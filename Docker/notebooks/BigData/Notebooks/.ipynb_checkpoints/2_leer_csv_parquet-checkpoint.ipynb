{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f4d9b2",
   "metadata": {},
   "source": [
    "# Leer CSV desde HDFS y convertir a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063265dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('LeerCSVParquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f13165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv('hdfs://namenode:8020/user/hadoop/ventas.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5835b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.printSchema()\n",
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.write.parquet('hdfs://namenode:8020/user/hadoop/output_parquet/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec223caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet('hdfs://namenode:8020/user/hadoop/output_parquet/')\n",
    "df_parquet.show(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
